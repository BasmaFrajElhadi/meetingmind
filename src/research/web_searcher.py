import sys
import os
from pathlib import Path
import logging
from dotenv import load_dotenv
from src.prompt_engineering.templates import load_prompt_template
from src.handlers.error_handler import handle_errors, MeetingMindError
from config.config_loader import load_model_config
from groq import Groq


# Ensure project root is available for imports
PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.append(str(PROJECT_ROOT))

# Load environment variables
load_dotenv()

# Logging configuration
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


class WebSearcher:
    """
    Performs web-based research using Groq LLM.

    Responsibilities:
    - Load a prompt template for web search.
    - Format the query using the template.
    - Execute the search via Groq LLM.
    - Return the research result as text.
    """

    def __init__(self):
        """
        Initialize the WebSearcher with a Groq client using the API key from environment variables.
        """
        config = load_model_config()["groq"]

        self.model_name = config["model_name"]
        self.temperature = config["temperature"]
        self.max_tokens = config["max_tokens"]

        self.client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

    @handle_errors("Failed to perform web search")
    def search(self, query: str) -> str:
        """
        Perform a web search based on the given query using Groq LLM.

        Args:
            query (str): The research query or topic.

        Returns:
            str: The summarized research result generated by the LLM.
        """
        # Load and format web search prompt template
        template = load_prompt_template(task_name="web_search")
        prompt = template.format(query=query)

        # Call the Groq LLM API
        chat_completion = self.client.chat.completions.create(
            model=self.model_name,
            messages=[{"role": "user", "content": prompt}],
            temperature=self.temperature,
            max_tokens=self.max_tokens
        )

        # Extract content from LLM response
        research_result = chat_completion.choices[0].message.content
        logging.info("Web search completed successfully")
        return research_result
